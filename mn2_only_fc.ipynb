{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6dbac15f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-11T20:44:05.601829Z",
     "start_time": "2022-04-11T20:44:05.589178Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "0f99e835",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-12T04:44:15.984605Z",
     "start_time": "2022-04-12T04:44:15.182137Z"
    }
   },
   "outputs": [],
   "source": [
    "from data_utils import WoodDataset\n",
    "from models import MBV2_CA\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.models as models\n",
    "from torch import nn\n",
    "import torch\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from os import path\n",
    "from tqdm.notebook import tqdm\n",
    "import joblib\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "3ed765f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-12T04:48:30.330588Z",
     "start_time": "2022-04-12T04:47:37.329214Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit: ········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/ivan/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6604daee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-11T20:45:56.450250Z",
     "start_time": "2022-04-11T20:44:56.786306Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2125ee3c235449da875f6816096ed9dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/266 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_100145/1322741780.py:31: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:210.)\n",
      "  mean = torch.mean(torch.tensor([m.numpy() for m in means]), dim=0)\n"
     ]
    }
   ],
   "source": [
    "train_val_test_split_params = {\n",
    "    'test_size': 0.25,\n",
    "    'valid_size': 0.25,\n",
    "    'random_state': 42,\n",
    "    'stratify': 'target'\n",
    "}\n",
    "\n",
    "stats_trans = transforms.Compose([\n",
    "                transforms.Resize(256),\n",
    "                transforms.CenterCrop(224),\n",
    "                transforms.ToTensor(),\n",
    "            ])\n",
    "\n",
    "train_dataset = WoodDataset(\n",
    "    img_dir='data', \n",
    "    is_test=False, \n",
    "    task_type='classification', \n",
    "    dataset_role='train', \n",
    "    train_val_test_split_params=train_val_test_split_params,\n",
    "    img_transforms=stats_trans\n",
    ")\n",
    "\n",
    "\n",
    "means = []\n",
    "stds = []\n",
    "for idx in tqdm(range(len(train_dataset)), total=len(train_dataset)):\n",
    "    img = train_dataset[idx]['image']\n",
    "    means.append(torch.mean(img, dim=(1, 2)))\n",
    "    stds.append(torch.std(img, dim=(1, 2)))\n",
    "\n",
    "mean = torch.mean(torch.tensor([m.numpy() for m in means]), dim=0)\n",
    "std = torch.mean(torch.tensor([s.numpy() for s in stds]), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "82b9619d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-12T05:26:37.249396Z",
     "start_time": "2022-04-12T05:26:37.229708Z"
    }
   },
   "outputs": [],
   "source": [
    "trans_with_aug = transforms.Compose([\n",
    "    transforms.AutoAugment(transforms.AutoAugmentPolicy.IMAGENET),\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std),\n",
    "])\n",
    "trans_without_aug = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "d1a9a905",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-12T05:26:37.565613Z",
     "start_time": "2022-04-12T05:26:37.525019Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = WoodDataset(\n",
    "    img_dir='data', \n",
    "    is_test=False, \n",
    "    task_type='ranking', \n",
    "    dataset_role='train', \n",
    "    train_val_test_split_params=train_val_test_split_params,\n",
    "    img_transforms=trans_with_aug\n",
    ")\n",
    "valid_dataset = WoodDataset(\n",
    "    img_dir='data', \n",
    "    is_test=False, \n",
    "    task_type='ranking', \n",
    "    dataset_role='valid', \n",
    "    train_val_test_split_params=train_val_test_split_params,\n",
    "    img_transforms=trans_with_aug\n",
    ")\n",
    "test_dataset = WoodDataset(\n",
    "    img_dir='data', \n",
    "    is_test=False, \n",
    "    task_type='ranking', \n",
    "    dataset_role='test', \n",
    "    train_val_test_split_params=train_val_test_split_params,\n",
    "    img_transforms=trans_without_aug\n",
    ")\n",
    "submission_dataset = WoodDataset(\n",
    "    img_dir='data', \n",
    "    is_test=True, \n",
    "    task_type='ranking',\n",
    "    img_transforms=trans_without_aug\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "52b29d27",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-12T17:44:19.597250Z",
     "start_time": "2022-04-12T17:44:19.580365Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_preset = {'batch_size': batch_size, 'shuffle': True, 'num_workers': 0, 'drop_last': True}\n",
    "test_preset = {'batch_size': 1, 'shuffle': False, 'num_workers': 0, 'drop_last': False}\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, **train_preset)\n",
    "valid_dataloader = DataLoader(valid_dataset, **train_preset)\n",
    "test_dataloader = DataLoader(test_dataset, **test_preset)\n",
    "submission_dataloader = DataLoader(submission_dataset, **test_preset)\n",
    "\n",
    "dataloaders = {\n",
    "    'train': train_dataloader, \n",
    "    'valid': valid_dataloader, \n",
    "    'test':test_dataloader, \n",
    "    'subm': submission_dataloader\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "37ebaab1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-12T05:26:39.486178Z",
     "start_time": "2022-04-12T05:26:39.445082Z"
    }
   },
   "outputs": [],
   "source": [
    "def layers_freeze(model):\n",
    "    for name, child in model.named_children():\n",
    "        for param in child.parameters():\n",
    "            param.requires_grad = False\n",
    "        layers_freeze(child)\n",
    "        \n",
    "        \n",
    "def layers_unfreeze(model):\n",
    "    for name, child in model.named_children():\n",
    "        for param in child.parameters():\n",
    "            param.requires_grad = True\n",
    "        layers_unfreeze(child)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "7837640d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-12T05:26:39.798437Z",
     "start_time": "2022-04-12T05:26:39.781814Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f2a9811a690>"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "d42caa23",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-12T05:26:42.668140Z",
     "start_time": "2022-04-12T05:26:42.595342Z"
    }
   },
   "outputs": [],
   "source": [
    "model = models.mobilenet_v2(pretrained=True)\n",
    "#layers_freeze(model)\n",
    "model.classifier = nn.Sequential(\n",
    "    model.classifier[0], \n",
    "    nn.Linear(model.classifier[1].in_features, 512, bias=True), \n",
    "    nn.ReLU6(), \n",
    "    nn.Linear(512, 2, bias=True),\n",
    "    nn.Sigmoid()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "d8005fc7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-12T05:27:00.076658Z",
     "start_time": "2022-04-12T05:27:00.052960Z"
    }
   },
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "\n",
    "loss = nn.MSELoss(reduction='sum')\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "#scheduler = lr_scheduler.MultiStepLR(optimizer, gamma=0.1, milestones=[5, 15, 30, 45])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "1316755d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-12T05:45:29.377909Z",
     "start_time": "2022-04-12T05:27:08.386042Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.14"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ivan/Рабочий стол/wood_classification/wandb/run-20220412_082708-apvqme2m</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/kepolol/wood%20hack/runs/apvqme2m\" target=\"_blank\">experiment_0</a></strong> to <a href=\"https://wandb.ai/kepolol/wood%20hack\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7f90c2fd13a4d65a17a661bc6500bed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ivan/Рабочий стол/wood_classification/data_utils.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {'image': in_img, 'target': torch.tensor(target, dtype=torch.float), 'img_path': path}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "191e7f2d70cd43bcad450f7cf395dd7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "287b6b70a34c4c9cac89fe27cbc713b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8721d5c76829440a83c4b0737075844d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1451cf9dc4c40de8522aed9f1553496",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c57594f22d84752b691bbb6dbe197f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00ee0ccdd54543d0b32668472af04992",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03bb4648598a4183aa30b1ad379bbd0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a741b316ddda46de8af85723bcbe5995",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "580064dd560b46e98ab335a478657254",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "689d7f9e28ee41bda9c62437f521f0fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bece2992542448cb82da943891d2508b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06a0e857c7dd4475b83548a4b602b3a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "487f7801efed41b499beee5fe06c5d30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "383f12df7e22481b803b9da9f671fc47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ded4e0cb04ec41e9b97fa460d3849bc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78536fdae0904c3e8cdff13c0332aca0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e063672c148449089aaeeb16c3fdb57d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbcf2b8f3a1841a89d865d7436a0734e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbcd54ace5a440d0978a8ddf706875a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "n_epochs = 10\n",
    "#es_num = 15\n",
    "\n",
    "train_epoch_losses = []\n",
    "valid_epoch_losses = []\n",
    "es_counter = 0\n",
    "\n",
    "wandb.init(\n",
    "    # Set the project where this run will be logged\n",
    "    project=\"wood hack\", \n",
    "    # We pass a run name (otherwise it’ll be randomly assigned, like sunshine-lollypop-10)\n",
    "    name=f\"experiment_0\", \n",
    "    # Track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": 'MultiStepLR(optimizer, gamma=0.1, milestones=[5, 15, 30, 45])',\n",
    "    \"architecture\": \"mobilenetv2 ranking without freezing\",\n",
    "    \"dataset\": \"ranking\",\n",
    "    \"epochs\": n_epochs,\n",
    "    })\n",
    "    \n",
    "    \n",
    "for epoch in range(n_epochs):\n",
    "    #if epoch == 6:\n",
    "    #    layers_unfreeze(model)\n",
    "    #    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    #    scheduler = lr_scheduler.MultiStepLR(optimizer, gamma=0.1, milestones=[10, 20, 30, 40])\n",
    "    for sample, dataloader in dataloaders.items():\n",
    "        if sample not in ['train', 'valid']:\n",
    "            continue\n",
    "        elif sample == 'train':\n",
    "            model.train()\n",
    "        elif sample == 'valid':\n",
    "            model.eval()\n",
    "        loss_val = 0\n",
    "        obj_cnt = 0\n",
    "        with tqdm(dataloader, unit=\"batch\") as tqdm_dataloader:\n",
    "            for batch in tqdm_train_dataloader:\n",
    "                tqdm_dataloader.set_description(f\"{sample} Epoch {epoch}\")\n",
    "                \n",
    "                images = batch['image'].to(device)\n",
    "                targets = batch['target'].to(device)\n",
    "                \n",
    "                batch_shape = images.shape[0]\n",
    "                obj_cnt += batch_shape\n",
    "                \n",
    "                model.zero_grad()\n",
    "                output = model(images)\n",
    "                batch_loss = loss(output, targets)\n",
    "                \n",
    "                if sample == 'train':\n",
    "                    batch_loss.backward()\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                batch_loss_val = batch_loss.detach().item()\n",
    "                loss_val += batch_loss_val\n",
    "                tqdm_dataloader.set_postfix(\n",
    "                    batch_loss=batch_loss_val / batch_shape,\n",
    "                    epoch_loss=loss_val / obj_cnt)\n",
    "                wandb.log({f'batch_{sample}_loss': batch_loss_val / batch_shape})\n",
    "        if sample == 'train':\n",
    "            train_epoch_losses.append(loss_val / obj_cnt)\n",
    "        elif sample == 'valid':\n",
    "            valid_epoch_losses.append(loss_val / obj_cnt)\n",
    "        wandb.log({f'epoch_{sample}_loss': loss_val / obj_cnt})\n",
    "    now_time_str = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    torch.save(model.state_dict(),\n",
    "               path.join(\n",
    "                   'data',\n",
    "                   'nn_chpt','mn2',\n",
    "                   f'model_{epoch}_{now_time_str}_{round(train_epoch_losses[-1], 4)}_{round(valid_epoch_losses[-1], 4)}'))\n",
    "wandb.finish()\n",
    "print(np.argmin(valid_epoch_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "a9216257",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-12T05:45:45.303280Z",
     "start_time": "2022-04-12T05:45:45.208492Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f2a74e63160>]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(10), valid_epoch_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "42a6147f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-12T06:05:38.648930Z",
     "start_time": "2022-04-12T06:05:06.967042Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c10111868f04296af90a8be1b11160e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/134 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ivan/Рабочий стол/wood_classification/data_utils.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {'image': in_img, 'target': torch.tensor(target, dtype=torch.float), 'img_path': path}\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    with tqdm(test_dataloader, unit=\"batch\") as tqdm_test_dataloader:\n",
    "        valid_loss = 0\n",
    "        valid_cnt = 0\n",
    "        pred = []\n",
    "        true = []\n",
    "        img_path = []\n",
    "        for batch in tqdm_test_dataloader:\n",
    "            images = batch['image'].to(device)\n",
    "            targets = batch['target'].to(device)\n",
    "            output = model(images)\n",
    "            output = np.argmax(np.hstack((\n",
    "                np.ones((output.shape[0], 1))*0.1, \n",
    "                ((output.numpy()>=0.4)[:,0]*0.2).reshape(-1, 1), \n",
    "                ((output.numpy()>=0.4)[:,1]*0.3).reshape(-1, 1))\n",
    "            ), axis=1)\n",
    "            pred.extend(output.tolist())\n",
    "            true.extend(targets.sum(dim=1).detach().numpy().tolist())\n",
    "            img_path.extend(batch['img_path'])\n",
    "test_df = pd.DataFrame({'true': true, 'pred': pred, 'img_path': img_path})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "27475955",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-12T18:12:11.723035Z",
     "start_time": "2022-04-12T18:12:11.692784Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.41      0.58        17\n",
      "         1.0       0.89      0.92      0.91        53\n",
      "         2.0       0.88      0.98      0.93        64\n",
      "\n",
      "    accuracy                           0.89       134\n",
      "   macro avg       0.92      0.77      0.81       134\n",
      "weighted avg       0.90      0.89      0.88       134\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_df.true, test_df.pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "5b960568",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-12T18:13:50.223410Z",
     "start_time": "2022-04-12T18:13:49.867280Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb4ad6cada034be5bbb4c4362cb879b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/249 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ivan/Рабочий стол/wood_classification/data_utils.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {'image': in_img, 'target': torch.tensor(target, dtype=torch.float), 'img_path': path}\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [147]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m             output \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(np\u001b[38;5;241m.\u001b[39mhstack((\n\u001b[1;32m     14\u001b[0m                 np\u001b[38;5;241m.\u001b[39mones((output\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m0.1\u001b[39m, \n\u001b[1;32m     15\u001b[0m                 ((output\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.4\u001b[39m)[:,\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m0.2\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m), \n\u001b[1;32m     16\u001b[0m                 ((output\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.4\u001b[39m)[:,\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m0.3\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     17\u001b[0m             ), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     18\u001b[0m             pred\u001b[38;5;241m.\u001b[39mextend(output\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[0;32m---> 19\u001b[0m             true\u001b[38;5;241m.\u001b[39mextend(\u001b[43mtargets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[1;32m     20\u001b[0m             img_path\u001b[38;5;241m.\u001b[39mextend(batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimg_path\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     21\u001b[0m test_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m'\u001b[39m: true, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpred\u001b[39m\u001b[38;5;124m'\u001b[39m: pred, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimg_path\u001b[39m\u001b[38;5;124m'\u001b[39m: img_path})\n",
      "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    with tqdm(dataloaders['subm'], unit=\"batch\") as tqdm_dataloader:\n",
    "        valid_loss = 0\n",
    "        valid_cnt = 0\n",
    "        pred = []\n",
    "        true = []\n",
    "        img_path = []\n",
    "        for batch in tqdm_dataloader:\n",
    "            images = batch['image'].to(device)\n",
    "            targets = batch['target'].to(device)\n",
    "            output = model(images)\n",
    "            output = np.argmax(np.hstack((\n",
    "                np.ones((output.shape[0], 1))*0.1, \n",
    "                ((output.numpy()>=0.4)[:,0]*0.2).reshape(-1, 1), \n",
    "                ((output.numpy()>=0.4)[:,1]*0.3).reshape(-1, 1))\n",
    "            ), axis=1)\n",
    "            pred.extend(output.tolist())\n",
    "            true.extend(targets.sum(dim=1).detach().numpy().tolist())\n",
    "            img_path.extend(batch['img_path'])\n",
    "test_df = pd.DataFrame({'true': true, 'pred': pred, 'img_path': img_path})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "783605ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-12T04:16:00.540883Z",
     "start_time": "2022-04-12T04:16:00.525315Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    134\n",
       "1     90\n",
       "2     25\n",
       "Name: pred, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.pred.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e11f4e9b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-12T04:16:43.028660Z",
     "start_time": "2022-04-12T04:16:43.013411Z"
    }
   },
   "outputs": [],
   "source": [
    "backward_mapping = {0: 1, 1: 3, 2: 0}\n",
    "\n",
    "test_df['id'] = test_df.img_path.map(lambda x: path.split(x)[-1].split('.')[0])\n",
    "test_df['class'] = test_df.pred.map(lambda x: backward_mapping[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6f99f20f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-12T04:16:53.157184Z",
     "start_time": "2022-04-12T04:16:53.140009Z"
    }
   },
   "outputs": [],
   "source": [
    "test_df[['id', 'class']].to_csv(path.join('data', 'mn_cls_99.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "d4867552",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-12T06:08:09.692695Z",
     "start_time": "2022-04-12T06:08:09.674369Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7,  5,  5],\n",
       "       [ 0, 49,  4],\n",
       "       [ 0,  1, 63]])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(test_df.true, test_df.pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855233ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
